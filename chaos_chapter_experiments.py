# %% [markdown]
# # Topological Divergences of Time Series
# 
# Data analysis for time series generated by chaotic maps, over a range of control parameter values, using various topological divergences. Classical Lyapunov estimators and recent TDA/HVG measures are also computed as baselines. 

# %% [markdown]
# ## Set up parallel processing
# 
# Ensure cluster is running before executing the code below.
# 
# Start a cluster with 32 cores with `ipcluster start -n 32`.
# 
# Ensure cluster is stopped after code is complete.
# 
# Stop the cluster with `ipcluster stop` in a separate terminal.

# %%
import ipyparallel as ipp
clients = ipp.Client()
dv = clients.direct_view()
lbv = clients.load_balanced_view()

# %% [markdown]
# ## Import modules, classes, and functions

# %%
import numpy as np
from scipy import stats

from LogisticMapLCE import logistic_lce
from HenonMapLCE import henon_lce
from IkedaMapLCE import ikeda_lce
from TinkerbellMapLCE import tinkerbell_lce

from TimeSeriesHVG import TimeSeriesHVG as TSHVG
from TimeSeriesMergeTree import TimeSeriesMergeTree as TSMT
from TimeSeriesPersistence import TimeSeriesPersistence as TSPH

from numpy.random import MT19937
from numpy.random import RandomState
from numpy.random import SeedSequence

# %% [markdown]
# ## Configure the experiment data

# %%
# draw samples from a known random state for reproducibility
SEED = 42
randomState = RandomState(MT19937(SeedSequence(SEED)))

TIME_SERIES_LENGTH = 200
NUM_CONTROL_PARAM_SAMPLES = 10

# %% [markdown]
# ### Logistic

# %%
logistic_control_params = [
    dict(r=r) for r in np.sort(randomState.uniform(3.5, 4.0, NUM_CONTROL_PARAM_SAMPLES))
]
logistic_dataset = [
    logistic_lce(mapParams=params, nIterates=TIME_SERIES_LENGTH, includeTrajectory=True)
    for params in logistic_control_params
]


# %% [markdown]
# ### Hénon

# %%
henon_control_params = [
    dict(a=a, b=0.3) for a in np.sort(randomState.uniform(0.8, 1.4, NUM_CONTROL_PARAM_SAMPLES))
]
henon_dataset = [
    henon_lce(mapParams=params, nIterates=TIME_SERIES_LENGTH, includeTrajectory=True)
    for params in henon_control_params
]


# %% [markdown]
# ### Ikeda

# %%
ikeda_control_params = [
    dict(a=a) for a in np.sort(randomState.uniform(0.5, 1.0, NUM_CONTROL_PARAM_SAMPLES))
]
ikeda_dataset = [
    ikeda_lce(mapParams=params, nIterates=TIME_SERIES_LENGTH, includeTrajectory=True)
    for params in ikeda_control_params
]


# %% [markdown]
# ### Tinkerbell

# %%
tinkerbell_control_params = [
    dict(a=a) for a in np.sort(randomState.uniform(0.7, 0.9, NUM_CONTROL_PARAM_SAMPLES))
]
tinkerbell_dataset = [
    tinkerbell_lce(mapParams=params, nIterates=TIME_SERIES_LENGTH, includeTrajectory=True)
    for params in tinkerbell_control_params
]


# %% [markdown]
# ## Build time series representations

# %%
def build_representation(dataset, rep_class, rep_class_kwargs):
    trajectories = [data["trajectory"][:,0] for data in dataset]
    return [rep_class(ts, **rep_class_kwargs) for ts in trajectories]

# %%
tshvg_kwargs = dict(
    DEGREE_DISTRIBUTION_MAX_DEGREE=100,
    DEGREE_DISTRIBUTION_DIVERGENCE_P_VALUE=1.0,
    directed=None,
    weighted=None,
    penetrable_limit=0,
)

# %%
tsmt_kwargs = dict(
    INTERLEAVING_DIVERGENCE_MESH=0.5,
    DMT_ALPHA=0.5,
    DISTRIBUTION_VECTOR_LENGTH=100,
    LEAF_NEIGHBOUR_OFFSET=1,
)

# %%
tsph_kwargs = dict(
    ENTROPY_SUMMARY_RESOLUTION=100,
    BETTI_CURVE_RESOLUTION=100,
    BETTI_CURVE_NORM_P_VALUE=1.0,
    SILHOUETTE_RESOLUTION=100,
    SILHOUETTE_WEIGHT=1,
    LIFESPAN_CURVE_RESOLUTION=100,
    IMAGE_BANDWIDTH=0.2,
    IMAGE_RESOLUTION=20,
    ENTROPY_SUMMARY_DIVERGENCE_P_VALUE=2.0,
    PERSISTENCE_STATISTICS_DIVERGENCE_P_VALUE=2.0,
    WASSERSTEIN_DIVERGENCE_P_VALUE=1.0,
    BETTI_CURVE_DIVERGENCE_P_VALUE=1.0,
    PERSISTENCE_SILHOUETTE_DIVERGENCE_P_VALUE=2.0,
    PERSISTENCE_LIFESPAN_DIVERGENCE_P_VALUE=2.0,
)

# %% [markdown]
# ### Logistic

# %%
logistic_tshvgs = build_representation(logistic_dataset, TSHVG, tshvg_kwargs)
logistic_tsmts = build_representation(logistic_dataset, TSMT, tsmt_kwargs)
logistic_tsphs = build_representation(logistic_dataset, TSPH, tsph_kwargs)

# %% [markdown]
# ### Hénon

# %%
henon_tshvgs = build_representation(henon_dataset, TSHVG, tshvg_kwargs)
henon_tsmts = build_representation(henon_dataset, TSMT, tsmt_kwargs)
henon_tsphs = build_representation(henon_dataset, TSPH, tsph_kwargs)

# %% [markdown]
# ### Ikeda

# %%
ikeda_tshvgs = build_representation(ikeda_dataset, TSHVG, tshvg_kwargs)
ikeda_tsmts = build_representation(ikeda_dataset, TSMT, tsmt_kwargs)
ikeda_tsphs = build_representation(ikeda_dataset, TSPH, tsph_kwargs)

# %% [markdown]
# ### Tinkerbell

# %%
tinkerbell_tshvgs = build_representation(tinkerbell_dataset, TSHVG, tshvg_kwargs)
tinkerbell_tsmts = build_representation(tinkerbell_dataset, TSMT, tsmt_kwargs)
tinkerbell_tsphs = build_representation(tinkerbell_dataset, TSPH, tsph_kwargs)

# %% [markdown]
# ## Get Lyapunov exponents and topological divergences

# %% [markdown]
# ### Lyapunov exponents (ground truth)
# 
# Calculated using numerical integration and the Benettin algorithm.

# %%
logistic_lces = np.array([data["lce"][0] for data in logistic_dataset])
henon_lces = np.array([data["lce"][0] for data in henon_dataset])
ikeda_lces = np.array([data["lce"][0] for data in ikeda_dataset])
tinkerbell_lces = np.array([data["lce"][0] for data in tinkerbell_dataset])

# %% [markdown]
# ### Helper functions

# %%
def dict_of_arrays(list_of_dicts):
    """Convert list of dictionaries with equal keys to a dictionary of numpy arrays.
    
    Example
        Input
            [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]
        Output
            {'a': np.array([1, 3]), 'b': np.array([2, 4])}
    """
    return {key: np.array([d[key] for d in list_of_dicts]) for key in list_of_dicts[0]}

def topological_divergences(ts_representations):
    divergences = dv.map_sync(lambda rep: rep.divergences, ts_representations)
    return dict_of_arrays(divergences)

# %% [markdown]
# ### HVG divergences
# 
# Wasserstein and $L_p$ divergences of the time series HVG degree distributions.

# %%
logistic_hvg_divergences = topological_divergences(logistic_tshvgs)
henon_hvg_divergences = topological_divergences(henon_tshvgs)
ikeda_hvg_divergences = topological_divergences(ikeda_tshvgs)
tinkerbell_hvg_divergences = topological_divergences(tinkerbell_tshvgs)

# %% [markdown]
# ### Merge tree divergences
# 
# Interleaving divergence and leaf-to-offset-leaf path length distribution divergence.

# %%
logistic_mt_divergences = topological_divergences(logistic_tsmts)
henon_mt_divergences = topological_divergences(henon_tsmts)
ikeda_mt_divergences = topological_divergences(ikeda_tsmts)
tinkerbell_mt_divergences = topological_divergences(tinkerbell_tsmts)

# %% [markdown]
# ### Persistent homology divergences
# 
# Various divergences based on the superlevel and sublevel persistence diagrams.

# %%
logistic_ph_divergences = topological_divergences(logistic_tsphs)
henon_ph_divergences = topological_divergences(henon_tsphs)
ikeda_ph_divergences = topological_divergences(ikeda_tsphs)
tinkerbell_ph_divergences = topological_divergences(tinkerbell_tsphs)

print(tinkerbell_hvg_divergences, tinkerbell_mt_divergences, tinkerbell_ph_divergences)
