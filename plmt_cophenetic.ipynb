{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Piecewise Linear Merge Tree Cophenetic Divergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from trajectories import generate_trajectories\n",
    "from plots_and_correlates import plot_lce_estimate_and_correlation\n",
    "from lca_supervised_learning import score_classification\n",
    "from lca_supervised_learning import score_regression\n",
    "from lca_supervised_learning import score_regression_pos\n",
    "from TimeSeriesMergeTreeSimple import TimeSeriesMergeTree as TSMT\n",
    "from ipyparallel import require\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyparallel as ipp\n",
    "clients = ipp.Client()\n",
    "dv = clients.direct_view()\n",
    "lbv = clients.load_balanced_view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS_NAMES = [\"henon\", \"ikeda\", \"logistic\", \"tinkerbell\"]\n",
    "DIV_TYPES = [\"dmt\", \"mt\", \"hvg\", \"ph\"]\n",
    "RES_TYPES = [\"correlations\", \"divergences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "SAMPLES = 500\n",
    "LENGTH = 250\n",
    "experimental_data = generate_trajectories(\n",
    "    RANDOM_SEED=SEED, TS_LENGTH=LENGTH, CONTROL_PARAM_SAMPLES=SAMPLES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monotonize(ts):\n",
    "    # forget intermediate non-critical points and equalize count of minima/maxima\n",
    "    new_ts = [ts[0]]\n",
    "    N = len(ts)\n",
    "    for idx in range(1,N-1):\n",
    "        x, y, z = ts[idx-1:idx+2]\n",
    "        if (((x<y) and (z<y)) or ((x>y) and (z>y))):\n",
    "            # add the local max/min\n",
    "            new_ts.append(y)\n",
    "    if (len(new_ts) % 2) == 1:\n",
    "        new_ts.append(ts[-1])\n",
    "\n",
    "    is_monotonic = lambda x: (np.all(x[::2]<x[1::2]) or np.all(x[::2]>x[1::2]))\n",
    "    assert is_monotonic(new_ts), \"new time series has non-critical values, somehow\"\n",
    "    return new_ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_trajectories = map(monotonize, experimental_data[\"logistic\"][\"trajectories\"])\n",
    "logistic_lces = experimental_data[\"logistic\"][\"lces\"]\n",
    "logistic_control_params = experimental_data[\"logistic\"][\"sys_params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "henon_trajectories = map(monotonize, experimental_data[\"henon\"][\"trajectories\"])\n",
    "henon_lces = experimental_data[\"henon\"][\"lces\"]\n",
    "henon_control_params = experimental_data[\"henon\"][\"sys_params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ikeda_trajectories = map(monotonize, experimental_data[\"ikeda\"][\"trajectories\"])\n",
    "ikeda_lces = experimental_data[\"ikeda\"][\"lces\"]\n",
    "ikeda_control_params = experimental_data[\"ikeda\"][\"sys_params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tinkerbell_trajectories = map(monotonize, experimental_data[\"tinkerbell\"][\"trajectories\"])\n",
    "tinkerbell_lces = experimental_data[\"tinkerbell\"][\"lces\"]\n",
    "tinkerbell_control_params = experimental_data[\"tinkerbell\"][\"sys_params\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build merge trees and compute divergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@require(np)\n",
    "def dict_of_arrays(list_of_dicts):\n",
    "    \"\"\"Convert list of dictionaries with equal keys to a dictionary of numpy arrays.\n",
    "    \n",
    "    Example\n",
    "        Input\n",
    "            [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]\n",
    "        Output\n",
    "            {'a': np.array([1, 3]), 'b': np.array([2, 4])}\n",
    "    \"\"\"\n",
    "    return {key: np.array([d[key] for d in list_of_dicts]) for key in list_of_dicts[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topological_divergences(ts_representations):\n",
    "    divergences = lbv.map_sync(lambda rep: rep.divergences, ts_representations)\n",
    "    return dict_of_arrays(divergences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plmt_estimates(\n",
    "    sys_name,\n",
    "    param_name,\n",
    "    trajectories,\n",
    "    control_params,\n",
    "    actual_lces,\n",
    "    show_plot=True,\n",
    "):\n",
    "    # store results to be returned\n",
    "    correlations_and_scores = {}\n",
    "\n",
    "    tsmts = map(TSMT, trajectories)\n",
    "    divergences = topological_divergences(tsmts)\n",
    "    for estimate_name, estimates in divergences.items():\n",
    "        # estimate_name = f\"pl_{estimate_name}\"\n",
    "        correlations_and_scores[\n",
    "            estimate_name, sys_name, LENGTH\n",
    "        ] = plot_lce_estimate_and_correlation(\n",
    "            estimate_name,\n",
    "            sys_name,\n",
    "            param_name,\n",
    "            estimates,\n",
    "            actual_lces,\n",
    "            control_params,\n",
    "            LENGTH,\n",
    "            show_plot=show_plot,\n",
    "            save_plot=True,\n",
    "            twoy=True,\n",
    "            plot_actual=True,\n",
    "        )\n",
    "        correlations_and_scores[\n",
    "            estimate_name, sys_name, LENGTH\n",
    "        ] |= {\n",
    "            \"classification_f1\": score_classification(estimates.reshape(-1,1), actual_lces),\n",
    "            \"regression_neg_mean_absolute\": score_regression(estimates.reshape(-1,1), actual_lces),\n",
    "            \"pos_regression_neg_mean_absolute\": score_regression_pos(estimates.reshape(-1,1), actual_lces)\n",
    "        }\n",
    "\n",
    "    return correlations_and_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_plmt = {}\n",
    "for sys_info in [\n",
    "    [\"Logistic\", \"r\", logistic_trajectories, logistic_control_params, logistic_lces],\n",
    "    [\"HÃ©non\", \"a\", henon_trajectories, henon_control_params, henon_lces],\n",
    "    [\"Tinkerbell\", \"a\", tinkerbell_trajectories, tinkerbell_control_params, tinkerbell_lces],\n",
    "    [\"Ikeda\", \"a\", ikeda_trajectories, ikeda_control_params, ikeda_lces],\n",
    "]:\n",
    "    all_results_plmt |= generate_plmt_estimates(*sys_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"outputs/data/PLMT_divergences_{LENGTH}.pkl\", \"wb\") as file:\n",
    "    pickle.dumps(all_results_plmt)\n",
    "\n",
    "\n",
    "for result in all_results_plmt:\n",
    "    print(result, all_results_plmt[result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chaos-chapter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
