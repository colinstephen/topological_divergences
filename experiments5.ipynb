{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topological Divergences of Time Series\n",
    "\n",
    "Data analysis for time series generated by chaotic maps, over a range of control parameter values, using various topological divergences. Classical Lyapunov estimators and recent TDA/HVG measures are also computed as baselines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up parallel processing\n",
    "\n",
    "Ensure cluster is running before executing the code below.\n",
    "\n",
    "Start a cluster with 32 cores with `ipcluster start -n 32`.\n",
    "\n",
    "Ensure cluster is stopped after code is complete.\n",
    "\n",
    "Stop the cluster with `ipcluster stop` in a separate terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyparallel as ipp\n",
    "clients = ipp.Client()\n",
    "dv = clients.direct_view()\n",
    "lbv = clients.load_balanced_view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clients.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules, classes, and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from dataclasses import asdict\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "\n",
    "from numpy.random import MT19937\n",
    "from numpy.random import RandomState\n",
    "from numpy.random import SeedSequence\n",
    "\n",
    "from nolds import lyap_r\n",
    "from nolds import lyap_e\n",
    "\n",
    "from teaspoon.SP import network_tools\n",
    "from teaspoon.SP.network import knn_graph\n",
    "from teaspoon.SP.network_tools import remove_zeros\n",
    "from teaspoon.SP.tsa_tools import takens\n",
    "from teaspoon.TDA.PHN import PH_network\n",
    "from teaspoon.TDA.PHN import point_summaries\n",
    "from gtda.time_series import takens_embedding_optimal_parameters\n",
    "\n",
    "from LogisticMapLCE import logistic_lce\n",
    "from HenonMapLCE import henon_lce\n",
    "from IkedaMapLCE import ikeda_lce\n",
    "from TinkerbellMapLCE import tinkerbell_lce\n",
    "\n",
    "from TimeSeriesHVG import TimeSeriesHVG as TSHVG\n",
    "from TimeSeriesMergeTreeSimple import TimeSeriesMergeTree as TSMT\n",
    "from TimeSeriesPersistence import TimeSeriesPersistence as TSPH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the experiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EXPERIMENT_CONFIG:\n",
    "    SEED = 42\n",
    "    RANDOM_STATE = RandomState(MT19937(SeedSequence(SEED)))\n",
    "    TIME_SERIES_LENGTH = 200\n",
    "    NUM_CONTROL_PARAM_SAMPLES = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allow converting configurations to dictionaries for saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configdict(cls):\n",
    "    \"\"\"Given a configuration class, convert it and its properties to a dictionary.\"\"\"\n",
    "    attributes = {attr: getattr(cls, attr) for attr in dir(cls) if not callable(getattr(cls, attr)) and not attr.startswith(\"__\")}\n",
    "    return {cls.__name__: attributes}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up wrapper for easy saving of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(filename, data, extra_metadata=None, RESULTS_DIR=\"outputs/data\"):\n",
    "    \"\"\"Save arbitrary data/results for future reference.\"\"\"\n",
    "\n",
    "    # use the experimental config to identify the file\n",
    "    filename_parts = [\n",
    "        f\"SEED_{EXPERIMENT_CONFIG.SEED}\",\n",
    "        f\"LENGTH_{EXPERIMENT_CONFIG.TIME_SERIES_LENGTH}\",\n",
    "        f\"SAMPLES_{EXPERIMENT_CONFIG.NUM_CONTROL_PARAM_SAMPLES}\",\n",
    "        filename,\n",
    "        f\"{datetime.utcnow()}\",\n",
    "    ]\n",
    "    full_filename = \"__\".join(filename_parts)\n",
    "    path = f\"{RESULTS_DIR}/{full_filename}.pkl\"\n",
    "\n",
    "    if extra_metadata is None:\n",
    "        extra_metadata = {}\n",
    "    metadata = configdict(EXPERIMENT_CONFIG) | extra_metadata\n",
    "\n",
    "    data_to_save = dict(\n",
    "        metadata=metadata,\n",
    "        data=data,\n",
    "    )\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(data_to_save, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation\n",
    "\n",
    "We $z$-normalise all sequences. This helps ensure divergence/distance hyper-parameters (such as the merge tree interleaving distance mesh size) are working at the same scale across the data sets considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_normalise(ts: np.array) -> np.array:\n",
    "    mean = np.mean(ts)\n",
    "    std = np.std(ts)\n",
    "    return (ts - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LOGISTIC_CONFIG:\n",
    "    R_MIN = 3.5\n",
    "    R_MAX = 4.0\n",
    "    TRAJECTORY_DIM = 0\n",
    "\n",
    "logistic_control_params = [\n",
    "    dict(r=r)\n",
    "    for r in np.sort(\n",
    "        EXPERIMENT_CONFIG.RANDOM_STATE.uniform(LOGISTIC_CONFIG.R_MIN, LOGISTIC_CONFIG.R_MAX, EXPERIMENT_CONFIG.NUM_CONTROL_PARAM_SAMPLES)\n",
    "    )\n",
    "]\n",
    "logistic_dataset = [\n",
    "    logistic_lce(mapParams=params, nIterates=EXPERIMENT_CONFIG.TIME_SERIES_LENGTH, includeTrajectory=True)\n",
    "    for params in logistic_control_params\n",
    "]\n",
    "logistic_trajectories = [\n",
    "    z_normalise(data[\"trajectory\"][:, LOGISTIC_CONFIG.TRAJECTORY_DIM])\n",
    "    for data in logistic_dataset\n",
    "]\n",
    "logistic_lces = np.array([data[\"lce\"][0] for data in logistic_dataset])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hénon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HENON_CONFIG:\n",
    "    A_MIN = 0.8\n",
    "    A_MAX = 1.4\n",
    "    B = 0.3\n",
    "    TRAJECTORY_DIM = 0\n",
    "\n",
    "henon_control_params = [\n",
    "    dict(a=a, b=HENON_CONFIG.B)\n",
    "    for a in np.sort(\n",
    "        EXPERIMENT_CONFIG.RANDOM_STATE.uniform(HENON_CONFIG.A_MIN, HENON_CONFIG.A_MAX, EXPERIMENT_CONFIG.NUM_CONTROL_PARAM_SAMPLES)\n",
    "    )\n",
    "]\n",
    "henon_dataset = [\n",
    "    henon_lce(mapParams=params, nIterates=EXPERIMENT_CONFIG.TIME_SERIES_LENGTH, includeTrajectory=True)\n",
    "    for params in henon_control_params\n",
    "]\n",
    "henon_trajectories = [\n",
    "    z_normalise(data[\"trajectory\"][:, HENON_CONFIG.TRAJECTORY_DIM]) for data in henon_dataset\n",
    "]\n",
    "henon_lces = np.array([data[\"lce\"][0] for data in henon_dataset])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ikeda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IKEDA_CONFIG:\n",
    "    A_MIN = 0.5\n",
    "    A_MAX = 1.0\n",
    "    TRAJECTORY_DIM = 0\n",
    "\n",
    "ikeda_control_params = [\n",
    "    dict(a=a)\n",
    "    for a in np.sort(\n",
    "        EXPERIMENT_CONFIG.RANDOM_STATE.uniform(IKEDA_CONFIG.A_MIN, IKEDA_CONFIG.A_MAX, EXPERIMENT_CONFIG.NUM_CONTROL_PARAM_SAMPLES)\n",
    "    )\n",
    "]\n",
    "ikeda_dataset = [\n",
    "    ikeda_lce(mapParams=params, nIterates=EXPERIMENT_CONFIG.TIME_SERIES_LENGTH, includeTrajectory=True)\n",
    "    for params in ikeda_control_params\n",
    "]\n",
    "ikeda_trajectories = [\n",
    "    z_normalise(data[\"trajectory\"][:, IKEDA_CONFIG.TRAJECTORY_DIM]) for data in ikeda_dataset\n",
    "]\n",
    "ikeda_lces = np.array([data[\"lce\"][0] for data in ikeda_dataset])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tinkerbell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TINKERBELL_CONFIG:\n",
    "    A_MIN = 0.7\n",
    "    A_MAX = 0.9\n",
    "    TRAJECTORY_DIM = 0\n",
    "\n",
    "tinkerbell_control_params = [\n",
    "    dict(a=a)\n",
    "    for a in np.sort(\n",
    "        EXPERIMENT_CONFIG.RANDOM_STATE.uniform(\n",
    "            TINKERBELL_CONFIG.A_MIN, TINKERBELL_CONFIG.A_MAX, EXPERIMENT_CONFIG.NUM_CONTROL_PARAM_SAMPLES\n",
    "        )\n",
    "    )\n",
    "]\n",
    "tinkerbell_dataset = [\n",
    "    tinkerbell_lce(\n",
    "        mapParams=params, nIterates=EXPERIMENT_CONFIG.TIME_SERIES_LENGTH, includeTrajectory=True\n",
    "    )\n",
    "    for params in tinkerbell_control_params\n",
    "]\n",
    "tinkerbell_trajectories = [\n",
    "    z_normalise(data[\"trajectory\"][:, TINKERBELL_CONFIG.TRAJECTORY_DIM])\n",
    "    for data in tinkerbell_dataset\n",
    "]\n",
    "tinkerbell_lces = np.array([data[\"lce\"][0] for data in tinkerbell_dataset])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build time series representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_representation(dataset, rep_class, rep_class_kwargs):\n",
    "    trajectories = [data[\"trajectory\"][:,0] for data in dataset]\n",
    "    return [rep_class(ts, **rep_class_kwargs) for ts in trajectories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "HVG_CONFIG = dict(\n",
    "    DEGREE_DISTRIBUTION_MAX_DEGREE=100,\n",
    "    DEGREE_DISTRIBUTION_DIVERGENCE_P_VALUE=1.0,\n",
    "    directed=None,\n",
    "    weighted=None,\n",
    "    penetrable_limit=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MT_CONFIG = dict(\n",
    "    INTERLEAVING_DIVERGENCE_MESH=0.25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PH_CONFIG = dict(\n",
    "    ENTROPY_SUMMARY_RESOLUTION=100,\n",
    "    BETTI_CURVE_RESOLUTION=100,\n",
    "    BETTI_CURVE_NORM_P_VALUE=1.0,\n",
    "    SILHOUETTE_RESOLUTION=100,\n",
    "    SILHOUETTE_WEIGHT=1,\n",
    "    LIFESPAN_CURVE_RESOLUTION=100,\n",
    "    IMAGE_BANDWIDTH=0.2,\n",
    "    IMAGE_RESOLUTION=20,\n",
    "    ENTROPY_SUMMARY_DIVERGENCE_P_VALUE=2.0,\n",
    "    PERSISTENCE_STATISTICS_DIVERGENCE_P_VALUE=2.0,\n",
    "    WASSERSTEIN_DIVERGENCE_P_VALUE=1.0,\n",
    "    BETTI_CURVE_DIVERGENCE_P_VALUE=1.0,\n",
    "    PERSISTENCE_SILHOUETTE_DIVERGENCE_P_VALUE=2.0,\n",
    "    PERSISTENCE_LIFESPAN_DIVERGENCE_P_VALUE=2.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_tshvgs = build_representation(logistic_dataset, TSHVG, HVG_CONFIG)\n",
    "logistic_tsmts = build_representation(logistic_dataset, TSMT, MT_CONFIG)\n",
    "logistic_tsphs = build_representation(logistic_dataset, TSPH, PH_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hénon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "henon_tshvgs = build_representation(henon_dataset, TSHVG, HVG_CONFIG)\n",
    "henon_tsmts = build_representation(henon_dataset, TSMT, MT_CONFIG)\n",
    "henon_tsphs = build_representation(henon_dataset, TSPH, PH_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ikeda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ikeda_tshvgs = build_representation(ikeda_dataset, TSHVG, HVG_CONFIG)\n",
    "ikeda_tsmts = build_representation(ikeda_dataset, TSMT, MT_CONFIG)\n",
    "ikeda_tsphs = build_representation(ikeda_dataset, TSPH, PH_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tinkerbell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tinkerbell_tshvgs = build_representation(tinkerbell_dataset, TSHVG, HVG_CONFIG)\n",
    "tinkerbell_tsmts = build_representation(tinkerbell_dataset, TSMT, MT_CONFIG)\n",
    "tinkerbell_tsphs = build_representation(tinkerbell_dataset, TSPH, PH_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Lyapunov exponents and topological divergences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lyapunov exponents (ground truth)\n",
    "\n",
    "Calculated using numerical integration and the Benettin algorithm above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_of_arrays(list_of_dicts):\n",
    "    \"\"\"Convert list of dictionaries with equal keys to a dictionary of numpy arrays.\n",
    "    \n",
    "    Example\n",
    "        Input\n",
    "            [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]\n",
    "        Output\n",
    "            {'a': np.array([1, 3]), 'b': np.array([2, 4])}\n",
    "    \"\"\"\n",
    "    return {key: np.array([d[key] for d in list_of_dicts]) for key in list_of_dicts[0]}\n",
    "\n",
    "def topological_divergences(ts_representations):\n",
    "    divergences = dv.map_sync(lambda rep: rep.divergences, ts_representations)\n",
    "    return dict_of_arrays(divergences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HVG divergences\n",
    "\n",
    "Wasserstein and $L_p$ divergences of the time series HVG degree distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_hvg_divergences = topological_divergences(logistic_tshvgs)\n",
    "save_result(\n",
    "    \"logistic_hvg_divergences\",\n",
    "    logistic_hvg_divergences,\n",
    "    {\n",
    "        \"logistic_config\": configdict(LOGISTIC_CONFIG),\n",
    "        \"hvg_config\": HVG_CONFIG,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "henon_hvg_divergences = topological_divergences(henon_tshvgs)\n",
    "save_result(\n",
    "    \"henon_hvg_divergences\",\n",
    "    henon_hvg_divergences,\n",
    "    {\n",
    "        \"henon_config\": configdict(HENON_CONFIG),\n",
    "        \"hvg_config\": HVG_CONFIG,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ikeda_hvg_divergences = topological_divergences(ikeda_tshvgs)\n",
    "save_result(\n",
    "    \"ikeda_hvg_divergences\",\n",
    "    ikeda_hvg_divergences,\n",
    "    {\n",
    "        \"ikeda_config\": configdict(IKEDA_CONFIG),\n",
    "        \"hvg_config\": HVG_CONFIG,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tinkerbell_hvg_divergences = topological_divergences(tinkerbell_tshvgs)\n",
    "save_result(\n",
    "    \"tinkerbell_hvg_divergences\",\n",
    "    tinkerbell_hvg_divergences,\n",
    "    {\n",
    "        \"tinkerbell_config\": configdict(TINKERBELL_CONFIG),\n",
    "        \"hvg_config\": HVG_CONFIG,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge tree divergences\n",
    "\n",
    "Interleaving divergence and leaf-to-offset-leaf path length distribution divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_mt_divergences = topological_divergences(logistic_tsmts)\n",
    "save_result(\n",
    "    \"logistic_mt_divergences\",\n",
    "    logistic_mt_divergences,\n",
    "    {\n",
    "        \"logistic_config\": configdict(LOGISTIC_CONFIG),\n",
    "        \"mt_config\": MT_CONFIG,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "henon_mt_divergences = topological_divergences(henon_tsmts)\n",
    "save_result(\n",
    "    \"henon_mt_divergences\",\n",
    "    henon_mt_divergences,\n",
    "    {\n",
    "        \"henon_config\": configdict(HENON_CONFIG),\n",
    "        \"mt_config\": MT_CONFIG,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ikeda_mt_divergences = topological_divergences(ikeda_tsmts)\n",
    "save_result(\n",
    "    \"ikeda_mt_divergences\",\n",
    "    ikeda_mt_divergences,\n",
    "    {\n",
    "        \"ikeda_config\": configdict(IKEDA_CONFIG),\n",
    "        \"mt_config\": MT_CONFIG,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tinkerbell_mt_divergences = topological_divergences(tinkerbell_tsmts)\n",
    "save_result(\n",
    "    \"tinkerbell_mt_divergences\",\n",
    "    tinkerbell_mt_divergences,\n",
    "    {\n",
    "        \"tinkerbell_config\": configdict(TINKERBELL_CONFIG),\n",
    "        \"mt_config\": MT_CONFIG,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistent homology divergences\n",
    "\n",
    "Various divergences based on the superlevel and sublevel persistence diagrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_ph_divergences = topological_divergences(logistic_tsphs)\n",
    "save_result(\n",
    "    \"logistic_ph_divergences\",\n",
    "    logistic_ph_divergences,\n",
    "    {\n",
    "        \"logistic_config\": configdict(LOGISTIC_CONFIG),\n",
    "        \"ph_config\": PH_CONFIG,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "henon_ph_divergences = topological_divergences(henon_tsphs)\n",
    "save_result(\n",
    "    \"henon_ph_divergences\",\n",
    "    henon_ph_divergences,\n",
    "    {\n",
    "        \"henon_config\": configdict(HENON_CONFIG),\n",
    "        \"ph_config\": PH_CONFIG,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ikeda_ph_divergences = topological_divergences(ikeda_tsphs)\n",
    "save_result(\n",
    "    \"ikeda_ph_divergences\",\n",
    "    ikeda_ph_divergences,\n",
    "    {\n",
    "        \"ikeda_config\": configdict(IKEDA_CONFIG),\n",
    "        \"ph_config\": PH_CONFIG,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tinkerbell_ph_divergences = topological_divergences(tinkerbell_tsphs)\n",
    "save_result(\n",
    "    \"tinkerbell_ph_divergences\",\n",
    "    tinkerbell_ph_divergences,\n",
    "    {\n",
    "        \"tinkerbell_config\": configdict(TINKERBELL_CONFIG),\n",
    "        \"ph_config\": PH_CONFIG,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations between divergences and Lyapunov exponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lyapunov_correlations(system_name, divergence_type, divergence_name, lces, divergences):\n",
    "    # find out how well a given estimator correlates with the lyapunov exponent\n",
    "    positive_mask = np.array(lces) > 0\n",
    "    return dict(\n",
    "        system_name = system_name,\n",
    "        divergence_type = divergence_type,\n",
    "        divergence_name = divergence_name,\n",
    "        pearsonr_full = stats.pearsonr(lces, divergences)[0],\n",
    "        spearmanr_full = stats.spearmanr(lces, divergences)[0],\n",
    "        kendalltau_full = stats.kendalltau(lces, divergences)[0],\n",
    "        pearsonr_pos = stats.pearsonr(lces[positive_mask], divergences[positive_mask])[0],\n",
    "        spearmanr_pos = stats.spearmanr(lces[positive_mask], divergences[positive_mask])[0],\n",
    "        kendalltau_pos = stats.kendalltau(lces[positive_mask], divergences[positive_mask])[0],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_correlation_results():\n",
    "    \"\"\"Build a dictionary of all system representation divergence correlations.\"\"\"\n",
    "\n",
    "    system_lces = {\n",
    "        \"logistic\": logistic_lces,\n",
    "        \"henon\": henon_lces,\n",
    "        \"ikeda\": ikeda_lces,\n",
    "        \"tinkerbell\": tinkerbell_lces,\n",
    "    }\n",
    "\n",
    "    divergence_results = {\n",
    "        \"logistic\": {\n",
    "            \"hvg\": logistic_hvg_divergences,\n",
    "            \"mt\": logistic_mt_divergences,\n",
    "            \"ph\": logistic_ph_divergences,\n",
    "        },\n",
    "        \"henon\": {\n",
    "            \"hvg\": henon_hvg_divergences,\n",
    "            \"mt\": henon_mt_divergences,\n",
    "            \"ph\": henon_ph_divergences,\n",
    "        },\n",
    "        \"ikeda\": {\n",
    "            \"hvg\": ikeda_hvg_divergences,\n",
    "            \"mt\": ikeda_mt_divergences,\n",
    "            \"ph\": ikeda_ph_divergences,\n",
    "        },\n",
    "        \"tinkerbell\": {\n",
    "            \"hvg\": tinkerbell_hvg_divergences,\n",
    "            \"mt\": tinkerbell_mt_divergences,\n",
    "            \"ph\": tinkerbell_ph_divergences,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    correlation_results = dict()\n",
    "\n",
    "    for sys_name in [\"logistic\", \"henon\", \"ikeda\", \"tinkerbell\"]:\n",
    "        lces = system_lces[sys_name]\n",
    "        for div_type in [\"hvg\", \"mt\", \"ph\"]:\n",
    "            for div_name, divs in divergence_results[sys_name][div_type].items():\n",
    "                correlation_results[\n",
    "                    sys_name, div_type, div_name\n",
    "                ] = lyapunov_correlations(sys_name, div_type, div_name, lces, divs)\n",
    "\n",
    "    return correlation_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m correlation_results \u001b[39m=\u001b[39m build_correlation_results()\n\u001b[1;32m      3\u001b[0m configs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m      4\u001b[0m configs \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m configdict(EXPERIMENT_CONFIG)\n",
      "Cell \u001b[0;32mIn[34], line 42\u001b[0m, in \u001b[0;36mbuild_correlation_results\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[39mfor\u001b[39;00m div_type \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mhvg\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mph\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m     39\u001b[0m         \u001b[39mfor\u001b[39;00m div_name, divs \u001b[39min\u001b[39;00m divergence_results[sys_name][div_type]\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     40\u001b[0m             correlation_results[\n\u001b[1;32m     41\u001b[0m                 sys_name, div_type, div_name\n\u001b[0;32m---> 42\u001b[0m             ] \u001b[39m=\u001b[39m lyapunov_correlations(sys_name, div_type, div_name, lces, divs)\n\u001b[1;32m     44\u001b[0m \u001b[39mreturn\u001b[39;00m correlation_results\n",
      "Cell \u001b[0;32mIn[33], line 8\u001b[0m, in \u001b[0;36mlyapunov_correlations\u001b[0;34m(system_name, divergence_type, divergence_name, lces, divergences)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlyapunov_correlations\u001b[39m(system_name, divergence_type, divergence_name, lces, divergences):\n\u001b[1;32m      2\u001b[0m     \u001b[39m# find out how well a given estimator correlates with the lyapunov exponent\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     positive_mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(lces) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mdict\u001b[39m(\n\u001b[1;32m      5\u001b[0m         system_name \u001b[39m=\u001b[39m system_name,\n\u001b[1;32m      6\u001b[0m         divergence_type \u001b[39m=\u001b[39m divergence_type,\n\u001b[1;32m      7\u001b[0m         divergence_name \u001b[39m=\u001b[39m divergence_name,\n\u001b[0;32m----> 8\u001b[0m         pearsonr_full \u001b[39m=\u001b[39m stats\u001b[39m.\u001b[39;49mpearsonr(lces, divergences)[\u001b[39m0\u001b[39m],\n\u001b[1;32m      9\u001b[0m         spearmanr_full \u001b[39m=\u001b[39m stats\u001b[39m.\u001b[39mspearmanr(lces, divergences)[\u001b[39m0\u001b[39m],\n\u001b[1;32m     10\u001b[0m         kendalltau_full \u001b[39m=\u001b[39m stats\u001b[39m.\u001b[39mkendalltau(lces, divergences)[\u001b[39m0\u001b[39m],\n\u001b[1;32m     11\u001b[0m         pearsonr_pos \u001b[39m=\u001b[39m stats\u001b[39m.\u001b[39mpearsonr(lces[positive_mask], divergences[positive_mask])[\u001b[39m0\u001b[39m],\n\u001b[1;32m     12\u001b[0m         spearmanr_pos \u001b[39m=\u001b[39m stats\u001b[39m.\u001b[39mspearmanr(lces[positive_mask], divergences[positive_mask])[\u001b[39m0\u001b[39m],\n\u001b[1;32m     13\u001b[0m         kendalltau_pos \u001b[39m=\u001b[39m stats\u001b[39m.\u001b[39mkendalltau(lces[positive_mask], divergences[positive_mask])[\u001b[39m0\u001b[39m],\n\u001b[1;32m     14\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/chaos-chapter/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4104\u001b[0m, in \u001b[0;36mpearsonr\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   4100\u001b[0m r \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(xm\u001b[39m/\u001b[39mnormxm, ym\u001b[39m/\u001b[39mnormym)\n\u001b[1;32m   4102\u001b[0m \u001b[39m# Presumably, if abs(r) > 1, then it is only some small artifact of\u001b[39;00m\n\u001b[1;32m   4103\u001b[0m \u001b[39m# floating point arithmetic.\u001b[39;00m\n\u001b[0;32m-> 4104\u001b[0m r \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mmin\u001b[39;49m(r, \u001b[39m1.0\u001b[39;49m), \u001b[39m-\u001b[39m\u001b[39m1.0\u001b[39m)\n\u001b[1;32m   4106\u001b[0m \u001b[39m# As explained in the docstring, the p-value can be computed as\u001b[39;00m\n\u001b[1;32m   4107\u001b[0m \u001b[39m#     p = 2*dist.cdf(-abs(r))\u001b[39;00m\n\u001b[1;32m   4108\u001b[0m \u001b[39m# where dist is the beta distribution on [-1, 1] with shape parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4112\u001b[0m \u001b[39m# becomes x = (-abs(r) + 1)/2 = 0.5*(1 - abs(r)).  (r is cast to float64\u001b[39;00m\n\u001b[1;32m   4113\u001b[0m \u001b[39m# to avoid a TypeError raised by btdtr when r is higher precision.)\u001b[39;00m\n\u001b[1;32m   4114\u001b[0m ab \u001b[39m=\u001b[39m n\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "correlation_results = build_correlation_results()\n",
    "\n",
    "configs = dict()\n",
    "configs |= configdict(EXPERIMENT_CONFIG)\n",
    "configs |= configdict(LOGISTIC_CONFIG)\n",
    "configs |= configdict(HENON_CONFIG)\n",
    "configs |= configdict(TINKERBELL_CONFIG)\n",
    "configs |= dict(MT_CONFIG=MT_CONFIG)\n",
    "configs |= dict(HVG_CONFIG=HVG_CONFIG)\n",
    "configs |= dict(PH_CONFIG=PH_CONFIG)\n",
    "\n",
    "save_result(\"correlation_results\", correlation_results, extra_metadata=configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots and visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_lce_and_topo_divergence(lces_to_plot, mt_div_to_plot):\n",
    "#     from matplotlib import pyplot as plt\n",
    "#     no_mask = lces_to_plot > -np.inf\n",
    "#     positive_mask = lces_to_plot > 0\n",
    "    \n",
    "#     pearsonr_full = stats.pearsonr(lces_to_plot, mt_div_to_plot)[0]\n",
    "#     spearmanr_full = stats.spearmanr(lces_to_plot, mt_div_to_plot)[0]\n",
    "#     kendalltau_full = stats.kendalltau(lces_to_plot, mt_div_to_plot)[0]\n",
    "#     pearsonr_pos = stats.pearsonr(lces_to_plot[positive_mask], mt_div_to_plot[positive_mask])[0]\n",
    "#     spearmanr_pos = stats.spearmanr(lces_to_plot[positive_mask], mt_div_to_plot[positive_mask])[0]\n",
    "#     kendalltau_pos = stats.kendalltau(lces_to_plot[positive_mask], mt_div_to_plot[positive_mask])[0]\n",
    "\n",
    "#     mask = no_mask\n",
    "#     plt.plot(lces_to_plot, lw=0.7)\n",
    "#     plt.show()\n",
    "#     plt.plot(mt_div_to_plot[mask], lw=0.7)\n",
    "#     plt.show()\n",
    "#     plt.scatter(lces_to_plot[mask], mt_div_to_plot[mask], s=1)\n",
    "#     plt.title(f\"P=({pearsonr_full}, {pearsonr_pos}),\\n S=({spearmanr_full}, {spearmanr_pos}),\\n T=({kendalltau_full}, {kendalltau_pos})\")\n",
    "#     plt.show()\n",
    "\n",
    "# plot_lce_and_topo_divergence(logistic_lces, logistic_hvg_divergences[\"degree_wasserstein\"])\n",
    "# plot_lce_and_topo_divergence(henon_lces, henon_hvg_divergences[\"degree_wasserstein\"])\n",
    "# plot_lce_and_topo_divergence(ikeda_lces, ikeda_hvg_divergences[\"degree_wasserstein\"])\n",
    "# plot_lce_and_topo_divergence(tinkerbell_lces, tinkerbell_hvg_divergences[\"degree_wasserstein\"])\n",
    "\n",
    "# plot_lce_and_topo_divergence(logistic_lces, logistic_mt_divergences[\"interleaving\"])\n",
    "# plot_lce_and_topo_divergence(henon_lces, henon_mt_divergences[\"interleaving\"])\n",
    "# plot_lce_and_topo_divergence(ikeda_lces, ikeda_mt_divergences[\"interleaving\"])\n",
    "# plot_lce_and_topo_divergence(tinkerbell_lces, tinkerbell_mt_divergences[\"interleaving\"])\n",
    "\n",
    "# plot_lce_and_topo_divergence(logistic_lces, logistic_mt_divergences[\"leaf_to_leaf_path_length\"][:,0])\n",
    "# plot_lce_and_topo_divergence(henon_lces, henon_mt_divergences[\"leaf_to_leaf_path_length\"][:,0])\n",
    "# plot_lce_and_topo_divergence(ikeda_lces, ikeda_mt_divergences[\"leaf_to_leaf_path_length\"][:,0])\n",
    "# plot_lce_and_topo_divergence(tinkerbell_lces, tinkerbell_mt_divergences[\"leaf_to_leaf_path_length\"][:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save numerical results so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data():\n",
    "\n",
    "    config = dict(\n",
    "        SEED=EXPERIMENT_CONFIG.SEED,\n",
    "        TIME_SERIES_LENGTH=EXPERIMENT_CONFIG.TIME_SERIES_LENGTH,\n",
    "        NUM_CONTROL_PARAM_SAMPLES=EXPERIMENT_CONFIG.NUM_CONTROL_PARAM_SAMPLES\n",
    "    )\n",
    "\n",
    "    hvg_divergences = dict(\n",
    "        tshvg_kwargs = HVG_CONFIG,\n",
    "        logistic=logistic_hvg_divergences,\n",
    "        henon=henon_hvg_divergences,\n",
    "        ikeda=ikeda_hvg_divergences,\n",
    "        tinkerbell=tinkerbell_hvg_divergences\n",
    "    )\n",
    "\n",
    "    mt_divergences = dict(\n",
    "        tsmt_kwargs = MT_CONFIG,\n",
    "        logistic=logistic_mt_divergences,\n",
    "        henon=henon_mt_divergences,\n",
    "        ikeda=ikeda_mt_divergences,\n",
    "        tinkerbell=tinkerbell_mt_divergences\n",
    "    )\n",
    "\n",
    "    ph_divergences = dict(\n",
    "        tsph_kwargs = PH_CONFIG,\n",
    "        logistic=logistic_ph_divergences,\n",
    "        henon=henon_ph_divergences,\n",
    "        ikeda=ikeda_ph_divergences,\n",
    "        tinkerbell=tinkerbell_ph_divergences\n",
    "    )\n",
    "\n",
    "    correlation_results = build_correlation_results()\n",
    "\n",
    "    data_to_save = dict(\n",
    "        config=config,\n",
    "        hvg_divergences=hvg_divergences,\n",
    "        mt_divergences=mt_divergences,\n",
    "        ph_divergences=ph_divergences,\n",
    "        correlation_results=correlation_results\n",
    "    )\n",
    "\n",
    "    filename = f\"outputs/data/topo_divs_{EXPERIMENT_CONFIG.SEED}_{EXPERIMENT_CONFIG.TIME_SERIES_LENGTH}_{EXPERIMENT_CONFIG.NUM_CONTROL_PARAM_SAMPLES}.pkl\"\n",
    "    with open(filename, \"wb\") as file:\n",
    "        pickle.dump(data_to_save, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "\n",
    "Other measures that might approximate or estimate the largest Lyapunov exponent of the trajectory data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical measures\n",
    "\n",
    "The Rosenstein and Eckmann estimates from Python `nolds`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic_rosenstein_estimates = np.array(dv.map_sync(lyap_r, logistic_trajectories))\n",
    "henon_rosenstein_estimates = np.array(dv.map_sync(lyap_r, henon_trajectories))\n",
    "ikeda_rosenstein_estimates = np.array(dv.map_sync(lyap_r, ikeda_trajectories))\n",
    "tinkerbell_rosenstein_estimates = np.array(dv.map_sync(lyap_r, tinkerbell_trajectories))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic_rosenstein_estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic_eckmann_estimates = np.array([x[0] for x in dv.map_sync(lyap_e, logistic_trajectories)])\n",
    "henon_eckmann_estimates = np.array([x[0] for x in dv.map_sync(lyap_e, henon_trajectories)])\n",
    "ikeda_eckmann_estimates = np.array([x[0] for x in dv.map_sync(lyap_e, ikeda_trajectories)])\n",
    "tinkerbell_eckmann_estimates = np.array([x[0] for x in dv.map_sync(lyap_e, tinkerbell_trajectories)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic_eckmann_estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HVG-based measures\n",
    "\n",
    "The $L_1$ distance between degree distributions of top and bottom HVGs. See Hasson _et. al._ (2018).\n",
    "\n",
    "This is already computed above as `logistic_hvg_divergences[\"degree_lp\"]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TDA-based measures\n",
    "\n",
    "1. The point summary statistics of persistent homology of kNN graphs of Takens embeddings of time series. See Myers _et. al._ (2019).\n",
    "2. The norm of Betti curves of the Vietorix Rips persistence on the full n-dimensional state space trajectory. See Güzel _et. al._ (2022). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def DistanceMatrixFixed(A):\n",
    "    \"\"\"Get the all-pairs unweighted shortest path lengths in the graph A.\n",
    "\n",
    "    Fixes an issue in the `teaspoon` library such that distance matrix computation\n",
    "    fails with disconnected graphs A.\n",
    "    \"\"\"\n",
    "\n",
    "    A = network_tools.remove_zeros(A)\n",
    "    np.fill_diagonal(A, 0)\n",
    "    A = A + A.T\n",
    "\n",
    "    A_sp = np.copy(A)\n",
    "    N = len(A_sp)\n",
    "    D = np.zeros((N,N))\n",
    "\n",
    "    A_sp[A_sp > 0] = 1\n",
    "    G = nx.from_numpy_matrix(A_sp)\n",
    "    lengths = dict(nx.all_pairs_shortest_path_length(G))    \n",
    "    for i in range(N-1):\n",
    "        for j in range(i+1, N):\n",
    "            D[i][j] = lengths.get(i, {}).get(j, np.inf)\n",
    "    D = D + D.T\n",
    "    return D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-NN persistence point summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMBED_MAX_DIM = 5\n",
    "EMBED_MAX_DELAY = 40\n",
    "\n",
    "logistic_optimal_embedding_params = [\n",
    "    takens_embedding_optimal_parameters(\n",
    "        ts, max_dimension=EMBED_MAX_DIM, max_time_delay=EMBED_MAX_DELAY\n",
    "    )\n",
    "    for ts in logistic_trajectories\n",
    "]\n",
    "henon_optimal_embedding_params = [\n",
    "    takens_embedding_optimal_parameters(\n",
    "        ts, max_dimension=EMBED_MAX_DIM, max_time_delay=EMBED_MAX_DELAY\n",
    "    )\n",
    "    for ts in henon_trajectories\n",
    "]\n",
    "ikeda_optimal_embedding_params = [\n",
    "    takens_embedding_optimal_parameters(\n",
    "        ts, max_dimension=EMBED_MAX_DIM, max_time_delay=EMBED_MAX_DELAY\n",
    "    )\n",
    "    for ts in ikeda_trajectories\n",
    "]\n",
    "tinkerbell_optimal_embedding_params = [\n",
    "    takens_embedding_optimal_parameters(\n",
    "        ts, max_dimension=EMBED_MAX_DIM, max_time_delay=EMBED_MAX_DELAY\n",
    "    )\n",
    "    for ts in tinkerbell_trajectories\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic_embeddings = [\n",
    "    takens(ts, n=dim, tau=tau)\n",
    "    for ts, (tau, dim) in zip(logistic_trajectories, logistic_optimal_embedding_params)\n",
    "]\n",
    "henon_embeddings = [\n",
    "    takens(ts, n=dim, tau=tau)\n",
    "    for ts, (tau, dim) in zip(henon_trajectories, henon_optimal_embedding_params)\n",
    "]\n",
    "ikeda_embeddings = [\n",
    "    takens(ts, n=dim, tau=tau)\n",
    "    for ts, (tau, dim) in zip(ikeda_trajectories, ikeda_optimal_embedding_params)\n",
    "]\n",
    "tinkerbell_embeddings = [\n",
    "    takens(ts, n=dim, tau=tau)\n",
    "    for ts, (tau, dim) in zip(\n",
    "        tinkerbell_trajectories, tinkerbell_optimal_embedding_params\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K_NEIGHBOURS = 4\n",
    "\n",
    "knn_graph_parallel = partial(knn_graph, k=K_NEIGHBOURS)\n",
    "\n",
    "logistic_knn_graphs = dv.map_sync(knn_graph_parallel, logistic_trajectories)\n",
    "henon_knn_graphs = dv.map_sync(knn_graph_parallel, henon_trajectories)\n",
    "ikeda_knn_graphs = dv.map_sync(knn_graph_parallel, ikeda_trajectories)\n",
    "tinkerbell_knn_graphs = dv.map_sync(knn_graph_parallel, tinkerbell_trajectories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic_knn_graphs = list(map(remove_zeros, logistic_knn_graphs))\n",
    "henon_knn_graphs = list(map(remove_zeros, henon_knn_graphs))\n",
    "ikeda_knn_graphs = list(map(remove_zeros, ikeda_knn_graphs))\n",
    "tinkerbell_knn_graphs = list(map(remove_zeros, tinkerbell_knn_graphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def distance_matrix_fixed(A):\n",
    "    \"\"\"Get the all-pairs unweighted shortest path lengths in the graph A.\n",
    "\n",
    "    Fixes an issue in the `teaspoon` library such that distance matrix computation\n",
    "    fails with disconnected graphs A.\n",
    "    \"\"\"\n",
    "\n",
    "    # include imports so function can run in ipyparallel workers\n",
    "    import numpy as np\n",
    "    import networkx as nx\n",
    "    from teaspoon.SP.network_tools import remove_zeros\n",
    "\n",
    "    A = remove_zeros(A)\n",
    "    np.fill_diagonal(A, 0)\n",
    "    A = A + A.T\n",
    "\n",
    "    A_sp = np.copy(A)\n",
    "    N = len(A_sp)\n",
    "    D = np.zeros((N,N))\n",
    "\n",
    "    A_sp[A_sp > 0] = 1\n",
    "    G = nx.from_numpy_matrix(A_sp)\n",
    "    lengths = dict(nx.all_pairs_shortest_path_length(G))    \n",
    "    for i in range(N-1):\n",
    "        for j in range(i+1, N):\n",
    "            D[i][j] = lengths.get(i, {}).get(j, np.inf)\n",
    "    D = D + D.T\n",
    "    return D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic_distance_matrices = dv.map_sync(distance_matrix_fixed, logistic_knn_graphs)\n",
    "henon_distance_matrices = dv.map_sync(distance_matrix_fixed, henon_knn_graphs)\n",
    "ikeda_distance_matrices = dv.map_sync(distance_matrix_fixed, ikeda_knn_graphs)\n",
    "tinkerbell_distance_matrices = dv.map_sync(distance_matrix_fixed, tinkerbell_knn_graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic_knn_diagrams = dv.map_sync(PH_network, logistic_distance_matrices)\n",
    "henon_knn_diagrams = dv.map_sync(PH_network, henon_distance_matrices)\n",
    "ikeda_knn_diagrams = dv.map_sync(PH_network, ikeda_distance_matrices)\n",
    "tinkerbell_knn_diagrams = dv.map_sync(PH_network, tinkerbell_distance_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic_knn_stats = dv.map_sync(point_summaries, logistic_knn_diagrams, logistic_knn_graphs)\n",
    "henon_knn_stats = dv.map_sync(point_summaries, henon_knn_diagrams, henon_knn_graphs)\n",
    "ikeda_knn_stats = dv.map_sync(point_summaries, ikeda_knn_diagrams, ikeda_knn_graphs)\n",
    "tinkerbell_knn_stats = dv.map_sync(point_summaries, tinkerbell_knn_diagrams, tinkerbell_knn_graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic_knn_stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chaos-chapter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
