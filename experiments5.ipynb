{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topological Divergences of Time Series\n",
    "\n",
    "Data analysis for time series generated by chaotic maps, over a range of control parameter values, using various topological divergences. Classical Lyapunov estimators and recent TDA/HVG measures are also computed as baselines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up parallel processing\n",
    "\n",
    "Ensure cluster is running before executing the code below.\n",
    "\n",
    "Start a cluster with 32 cores with `ipcluster start -n 32`.\n",
    "\n",
    "Ensure cluster is stopped after code is complete.\n",
    "\n",
    "Stop the cluster with `ipcluster stop` in a separate terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyparallel as ipp\n",
    "clients = ipp.Client()\n",
    "dv = clients.direct_view()\n",
    "lbv = clients.load_balanced_view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules, classes, and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from scipy import stats\n",
    "from functools import partial\n",
    "\n",
    "from numpy.random import MT19937\n",
    "from numpy.random import RandomState\n",
    "from numpy.random import SeedSequence\n",
    "\n",
    "from nolds import lyap_r\n",
    "from nolds import lyap_e\n",
    "\n",
    "from teaspoon.SP import network_tools\n",
    "from teaspoon.SP.network import ordinal_partition_graph\n",
    "from teaspoon.SP.network import knn_graph\n",
    "from teaspoon.SP.network_tools import remove_zeros\n",
    "from teaspoon.SP.network_tools import make_network\n",
    "from teaspoon.SP.tsa_tools import takens\n",
    "from teaspoon.parameter_selection.MsPE import MsPE_tau\n",
    "from teaspoon.TDA.Persistence import BettiCurve\n",
    "from teaspoon.TDA.PHN import DistanceMatrix\n",
    "from teaspoon.TDA.PHN import PH_network\n",
    "from teaspoon.TDA.PHN import point_summaries\n",
    "from gtda.time_series import takens_embedding_optimal_parameters\n",
    "\n",
    "from LogisticMapLCE import logistic_lce\n",
    "from HenonMapLCE import henon_lce\n",
    "from IkedaMapLCE import ikeda_lce\n",
    "from TinkerbellMapLCE import tinkerbell_lce\n",
    "\n",
    "from TimeSeriesHVG import TimeSeriesHVG as TSHVG\n",
    "from TimeSeriesMergeTreeSimple import TimeSeriesMergeTree as TSMT\n",
    "from TimeSeriesPersistence import TimeSeriesPersistence as TSPH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the experiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw samples from a known random state for reproducibility\n",
    "SEED = 42\n",
    "randomState = RandomState(MT19937(SeedSequence(SEED)))\n",
    "\n",
    "TIME_SERIES_LENGTH = 1000\n",
    "NUM_CONTROL_PARAM_SAMPLES = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_control_params = [\n",
    "    dict(r=r) for r in np.sort(randomState.uniform(3.5, 4.0, NUM_CONTROL_PARAM_SAMPLES))\n",
    "]\n",
    "logistic_dataset = [\n",
    "    logistic_lce(mapParams=params, nIterates=TIME_SERIES_LENGTH, includeTrajectory=True)\n",
    "    for params in logistic_control_params\n",
    "]\n",
    "logistic_trajectories = [\n",
    "    data[\"trajectory\"][:,0]\n",
    "    for data in logistic_dataset\n",
    "]\n",
    "logistic_lces = np.array([data[\"lce\"][0] for data in logistic_dataset])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hénon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "henon_control_params = [\n",
    "    dict(a=a, b=0.3) for a in np.sort(randomState.uniform(0.8, 1.4, NUM_CONTROL_PARAM_SAMPLES))\n",
    "]\n",
    "henon_dataset = [\n",
    "    henon_lce(mapParams=params, nIterates=TIME_SERIES_LENGTH, includeTrajectory=True)\n",
    "    for params in henon_control_params\n",
    "]\n",
    "henon_trajectories = [\n",
    "    data[\"trajectory\"][:,1]\n",
    "    for data in henon_dataset\n",
    "]\n",
    "henon_lces = np.array([data[\"lce\"][0] for data in henon_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ikeda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ikeda_control_params = [\n",
    "    dict(a=a) for a in np.sort(randomState.uniform(0.5, 1.0, NUM_CONTROL_PARAM_SAMPLES))\n",
    "]\n",
    "ikeda_dataset = [\n",
    "    ikeda_lce(mapParams=params, nIterates=TIME_SERIES_LENGTH, includeTrajectory=True)\n",
    "    for params in ikeda_control_params\n",
    "]\n",
    "ikeda_trajectories = [\n",
    "    data[\"trajectory\"][:,0]\n",
    "    for data in ikeda_dataset\n",
    "]\n",
    "ikeda_lces = np.array([data[\"lce\"][0] for data in ikeda_dataset])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tinkerbell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tinkerbell_control_params = [\n",
    "    dict(a=a) for a in np.sort(randomState.uniform(0.7, 0.9, NUM_CONTROL_PARAM_SAMPLES))\n",
    "]\n",
    "tinkerbell_dataset = [\n",
    "    tinkerbell_lce(mapParams=params, nIterates=TIME_SERIES_LENGTH, includeTrajectory=True)\n",
    "    for params in tinkerbell_control_params\n",
    "]\n",
    "tinkerbell_trajectories = [\n",
    "    data[\"trajectory\"][:,0]\n",
    "    for data in tinkerbell_dataset\n",
    "]\n",
    "tinkerbell_lces = np.array([data[\"lce\"][0] for data in tinkerbell_dataset])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build time series representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_representation(dataset, rep_class, rep_class_kwargs):\n",
    "    trajectories = [data[\"trajectory\"][:,0] for data in dataset]\n",
    "    return [rep_class(ts, **rep_class_kwargs) for ts in trajectories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tshvg_kwargs = dict(\n",
    "    DEGREE_DISTRIBUTION_MAX_DEGREE=100,\n",
    "    DEGREE_DISTRIBUTION_DIVERGENCE_P_VALUE=1.0,\n",
    "    directed=None,\n",
    "    weighted=None,\n",
    "    penetrable_limit=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsmt_kwargs = dict(\n",
    "    INTERLEAVING_DIVERGENCE_MESH=0.25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsph_kwargs = dict(\n",
    "    ENTROPY_SUMMARY_RESOLUTION=100,\n",
    "    BETTI_CURVE_RESOLUTION=100,\n",
    "    BETTI_CURVE_NORM_P_VALUE=1.0,\n",
    "    SILHOUETTE_RESOLUTION=100,\n",
    "    SILHOUETTE_WEIGHT=1,\n",
    "    LIFESPAN_CURVE_RESOLUTION=100,\n",
    "    IMAGE_BANDWIDTH=0.2,\n",
    "    IMAGE_RESOLUTION=20,\n",
    "    ENTROPY_SUMMARY_DIVERGENCE_P_VALUE=2.0,\n",
    "    PERSISTENCE_STATISTICS_DIVERGENCE_P_VALUE=2.0,\n",
    "    WASSERSTEIN_DIVERGENCE_P_VALUE=1.0,\n",
    "    BETTI_CURVE_DIVERGENCE_P_VALUE=1.0,\n",
    "    PERSISTENCE_SILHOUETTE_DIVERGENCE_P_VALUE=2.0,\n",
    "    PERSISTENCE_LIFESPAN_DIVERGENCE_P_VALUE=2.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_tshvgs = build_representation(logistic_dataset, TSHVG, tshvg_kwargs)\n",
    "logistic_tsmts = build_representation(logistic_dataset, TSMT, tsmt_kwargs)\n",
    "logistic_tsphs = build_representation(logistic_dataset, TSPH, tsph_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hénon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "henon_tshvgs = build_representation(henon_dataset, TSHVG, tshvg_kwargs)\n",
    "henon_tsmts = build_representation(henon_dataset, TSMT, tsmt_kwargs)\n",
    "henon_tsphs = build_representation(henon_dataset, TSPH, tsph_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ikeda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ikeda_tshvgs = build_representation(ikeda_dataset, TSHVG, tshvg_kwargs)\n",
    "ikeda_tsmts = build_representation(ikeda_dataset, TSMT, tsmt_kwargs)\n",
    "ikeda_tsphs = build_representation(ikeda_dataset, TSPH, tsph_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tinkerbell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tinkerbell_tshvgs = build_representation(tinkerbell_dataset, TSHVG, tshvg_kwargs)\n",
    "tinkerbell_tsmts = build_representation(tinkerbell_dataset, TSMT, tsmt_kwargs)\n",
    "tinkerbell_tsphs = build_representation(tinkerbell_dataset, TSPH, tsph_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Lyapunov exponents and topological divergences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lyapunov exponents (ground truth)\n",
    "\n",
    "Calculated using numerical integration and the Benettin algorithm above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_of_arrays(list_of_dicts):\n",
    "    \"\"\"Convert list of dictionaries with equal keys to a dictionary of numpy arrays.\n",
    "    \n",
    "    Example\n",
    "        Input\n",
    "            [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]\n",
    "        Output\n",
    "            {'a': np.array([1, 3]), 'b': np.array([2, 4])}\n",
    "    \"\"\"\n",
    "    return {key: np.array([d[key] for d in list_of_dicts]) for key in list_of_dicts[0]}\n",
    "\n",
    "def topological_divergences(ts_representations):\n",
    "    divergences = dv.map_sync(lambda rep: rep.divergences, ts_representations)\n",
    "    return dict_of_arrays(divergences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HVG divergences\n",
    "\n",
    "Wasserstein and $L_p$ divergences of the time series HVG degree distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_hvg_divergences = topological_divergences(logistic_tshvgs)\n",
    "henon_hvg_divergences = topological_divergences(henon_tshvgs)\n",
    "ikeda_hvg_divergences = topological_divergences(ikeda_tshvgs)\n",
    "tinkerbell_hvg_divergences = topological_divergences(tinkerbell_tshvgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge tree divergences\n",
    "\n",
    "Interleaving divergence and leaf-to-offset-leaf path length distribution divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_mt_divergences = topological_divergences(logistic_tsmts)\n",
    "henon_mt_divergences = topological_divergences(henon_tsmts)\n",
    "ikeda_mt_divergences = topological_divergences(ikeda_tsmts)\n",
    "tinkerbell_mt_divergences = topological_divergences(tinkerbell_tsmts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(henon_lces, lw=0.7)\n",
    "plt.show()\n",
    "plt.plot([div[\"interleaving\"] for div in henon_mt_divergences])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "henon_mt_divergences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistent homology divergences\n",
    "\n",
    "Various divergences based on the superlevel and sublevel persistence diagrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_ph_divergences = topological_divergences(logistic_tsphs)\n",
    "henon_ph_divergences = topological_divergences(henon_tsphs)\n",
    "ikeda_ph_divergences = topological_divergences(ikeda_tsphs)\n",
    "tinkerbell_ph_divergences = topological_divergences(tinkerbell_tsphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_ph_divergences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "\n",
    "Other measures that might approximate or estimate the largest Lyapunov exponent of the trajectory data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical measures\n",
    "\n",
    "The Rosenstein and Eckmann estimates from Python `nolds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_rosenstein_estimates = np.array(dv.map_sync(lyap_r, logistic_trajectories))\n",
    "henon_rosenstein_estimates = np.array(dv.map_sync(lyap_r, henon_trajectories))\n",
    "ikeda_rosenstein_estimates = np.array(dv.map_sync(lyap_r, ikeda_trajectories))\n",
    "tinkerbell_rosenstein_estimates = np.array(dv.map_sync(lyap_r, tinkerbell_trajectories))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_rosenstein_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_eckmann_estimates = np.array([x[0] for x in dv.map_sync(lyap_e, logistic_trajectories)])\n",
    "henon_eckmann_estimates = np.array([x[0] for x in dv.map_sync(lyap_e, henon_trajectories)])\n",
    "ikeda_eckmann_estimates = np.array([x[0] for x in dv.map_sync(lyap_e, ikeda_trajectories)])\n",
    "tinkerbell_eckmann_estimates = np.array([x[0] for x in dv.map_sync(lyap_e, tinkerbell_trajectories)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_eckmann_estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HVG-based measures\n",
    "\n",
    "The $L_1$ distance between degree distributions of top and bottom HVGs. See Hasson _et. al._ (2018).\n",
    "\n",
    "This is already computed above as `logistic_hvg_divergences[\"degree_lp\"]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TDA-based measures\n",
    "\n",
    "1. The point summary statistics of persistent homology of kNN graphs of Takens embeddings of time series. See Myers _et. al._ (2019).\n",
    "2. The norm of Betti curves of the Vietorix Rips persistence on the full n-dimensional state space trajectory. See Güzel _et. al._ (2022). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DistanceMatrixFixed(A):\n",
    "    \"\"\"Get the all-pairs unweighted shortest path lengths in the graph A.\n",
    "\n",
    "    Fixes an issue in the `teaspoon` library such that distance matrix computation\n",
    "    fails with disconnected graphs A.\n",
    "    \"\"\"\n",
    "\n",
    "    A = network_tools.remove_zeros(A)\n",
    "    np.fill_diagonal(A, 0)\n",
    "    A = A + A.T\n",
    "\n",
    "    A_sp = np.copy(A)\n",
    "    N = len(A_sp)\n",
    "    D = np.zeros((N,N))\n",
    "\n",
    "    A_sp[A_sp > 0] = 1\n",
    "    G = nx.from_numpy_matrix(A_sp)\n",
    "    lengths = dict(nx.all_pairs_shortest_path_length(G))    \n",
    "    for i in range(N-1):\n",
    "        for j in range(i+1, N):\n",
    "            D[i][j] = lengths.get(i, {}).get(j, np.inf)\n",
    "    D = D + D.T\n",
    "    return D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-NN persistence point summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_MAX_DIM = 5\n",
    "EMBED_MAX_DELAY = 40\n",
    "\n",
    "logistic_optimal_embedding_params = [\n",
    "    takens_embedding_optimal_parameters(\n",
    "        ts, max_dimension=EMBED_MAX_DIM, max_time_delay=EMBED_MAX_DELAY\n",
    "    )\n",
    "    for ts in logistic_trajectories\n",
    "]\n",
    "henon_optimal_embedding_params = [\n",
    "    takens_embedding_optimal_parameters(\n",
    "        ts, max_dimension=EMBED_MAX_DIM, max_time_delay=EMBED_MAX_DELAY\n",
    "    )\n",
    "    for ts in henon_trajectories\n",
    "]\n",
    "ikeda_optimal_embedding_params = [\n",
    "    takens_embedding_optimal_parameters(\n",
    "        ts, max_dimension=EMBED_MAX_DIM, max_time_delay=EMBED_MAX_DELAY\n",
    "    )\n",
    "    for ts in ikeda_trajectories\n",
    "]\n",
    "tinkerbell_optimal_embedding_params = [\n",
    "    takens_embedding_optimal_parameters(\n",
    "        ts, max_dimension=EMBED_MAX_DIM, max_time_delay=EMBED_MAX_DELAY\n",
    "    )\n",
    "    for ts in tinkerbell_trajectories\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_embeddings = [\n",
    "    takens(ts, n=dim, tau=tau)\n",
    "    for ts, (tau, dim) in zip(logistic_trajectories, logistic_optimal_embedding_params)\n",
    "]\n",
    "henon_embeddings = [\n",
    "    takens(ts, n=dim, tau=tau)\n",
    "    for ts, (tau, dim) in zip(henon_trajectories, henon_optimal_embedding_params)\n",
    "]\n",
    "ikeda_embeddings = [\n",
    "    takens(ts, n=dim, tau=tau)\n",
    "    for ts, (tau, dim) in zip(ikeda_trajectories, ikeda_optimal_embedding_params)\n",
    "]\n",
    "tinkerbell_embeddings = [\n",
    "    takens(ts, n=dim, tau=tau)\n",
    "    for ts, (tau, dim) in zip(\n",
    "        tinkerbell_trajectories, tinkerbell_optimal_embedding_params\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_NEIGHBOURS = 4\n",
    "\n",
    "knn_graph_parallel = partial(knn_graph, k=K_NEIGHBOURS)\n",
    "\n",
    "logistic_knn_graphs = dv.map_sync(knn_graph_parallel, logistic_trajectories)\n",
    "henon_knn_graphs = dv.map_sync(knn_graph_parallel, henon_trajectories)\n",
    "ikeda_knn_graphs = dv.map_sync(knn_graph_parallel, ikeda_trajectories)\n",
    "tinkerbell_knn_graphs = dv.map_sync(knn_graph_parallel, tinkerbell_trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_knn_graphs = list(map(remove_zeros, logistic_knn_graphs))\n",
    "henon_knn_graphs = list(map(remove_zeros, henon_knn_graphs))\n",
    "ikeda_knn_graphs = list(map(remove_zeros, ikeda_knn_graphs))\n",
    "tinkerbell_knn_graphs = list(map(remove_zeros, tinkerbell_knn_graphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_matrix_fixed(A):\n",
    "    \"\"\"Get the all-pairs unweighted shortest path lengths in the graph A.\n",
    "\n",
    "    Fixes an issue in the `teaspoon` library such that distance matrix computation\n",
    "    fails with disconnected graphs A.\n",
    "    \"\"\"\n",
    "\n",
    "    # include imports so function can run in ipyparallel workers\n",
    "    import numpy as np\n",
    "    import networkx as nx\n",
    "    from teaspoon.SP.network_tools import remove_zeros\n",
    "\n",
    "    A = remove_zeros(A)\n",
    "    np.fill_diagonal(A, 0)\n",
    "    A = A + A.T\n",
    "\n",
    "    A_sp = np.copy(A)\n",
    "    N = len(A_sp)\n",
    "    D = np.zeros((N,N))\n",
    "\n",
    "    A_sp[A_sp > 0] = 1\n",
    "    G = nx.from_numpy_matrix(A_sp)\n",
    "    lengths = dict(nx.all_pairs_shortest_path_length(G))    \n",
    "    for i in range(N-1):\n",
    "        for j in range(i+1, N):\n",
    "            D[i][j] = lengths.get(i, {}).get(j, np.inf)\n",
    "    D = D + D.T\n",
    "    return D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_distance_matrices = dv.map_sync(distance_matrix_fixed, logistic_knn_graphs)\n",
    "henon_distance_matrices = dv.map_sync(distance_matrix_fixed, henon_knn_graphs)\n",
    "ikeda_distance_matrices = dv.map_sync(distance_matrix_fixed, ikeda_knn_graphs)\n",
    "tinkerbell_distance_matrices = dv.map_sync(distance_matrix_fixed, tinkerbell_knn_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_knn_diagrams = dv.map_sync(PH_network, logistic_distance_matrices)\n",
    "henon_knn_diagrams = dv.map_sync(PH_network, henon_distance_matrices)\n",
    "ikeda_knn_diagrams = dv.map_sync(PH_network, ikeda_distance_matrices)\n",
    "tinkerbell_knn_diagrams = dv.map_sync(PH_network, tinkerbell_distance_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_knn_stats = dv.map_sync(point_summaries, logistic_knn_diagrams, logistic_knn_graphs)\n",
    "henon_knn_stats = dv.map_sync(point_summaries, henon_knn_diagrams, henon_knn_graphs)\n",
    "ikeda_knn_stats = dv.map_sync(point_summaries, ikeda_knn_diagrams, ikeda_knn_graphs)\n",
    "tinkerbell_knn_stats = dv.map_sync(point_summaries, tinkerbell_knn_diagrams, tinkerbell_knn_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_knn_stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chaos-chapter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
