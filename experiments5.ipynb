{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topological Divergences of Time Series\n",
    "\n",
    "Data analysis for time series generated by chaotic maps, over a range of control parameter values, using various topological divergences. Classical Lyapunov estimators and recent TDA/HVG measures are also computed as baselines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up parallel processing\n",
    "\n",
    "Ensure cluster is running before executing the code below.\n",
    "\n",
    "Start a cluster with 32 cores with `ipcluster start -n 32`.\n",
    "\n",
    "Ensure cluster is stopped after code is complete.\n",
    "\n",
    "Stop the cluster with `ipcluster stop` in a separate terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyparallel as ipp\n",
    "clients = ipp.Client()\n",
    "dv = clients.direct_view()\n",
    "lbv = clients.load_balanced_view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules, classes, and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from numpy.random import MT19937\n",
    "from numpy.random import RandomState\n",
    "from numpy.random import SeedSequence\n",
    "\n",
    "from nolds import lyap_r\n",
    "from nolds import lyap_e\n",
    "\n",
    "from LogisticMapLCE import logistic_lce\n",
    "from HenonMapLCE import henon_lce\n",
    "from IkedaMapLCE import ikeda_lce\n",
    "from TinkerbellMapLCE import tinkerbell_lce\n",
    "\n",
    "from TimeSeriesHVG import TimeSeriesHVG as TSHVG\n",
    "from TimeSeriesMergeTree import TimeSeriesMergeTree as TSMT\n",
    "from TimeSeriesPersistence import TimeSeriesPersistence as TSPH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the experiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw samples from a known random state for reproducibility\n",
    "SEED = 42\n",
    "randomState = RandomState(MT19937(SeedSequence(SEED)))\n",
    "\n",
    "TIME_SERIES_LENGTH = 200\n",
    "NUM_CONTROL_PARAM_SAMPLES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_control_params = [\n",
    "    dict(r=r) for r in np.sort(randomState.uniform(3.5, 4.0, NUM_CONTROL_PARAM_SAMPLES))\n",
    "]\n",
    "logistic_dataset = [\n",
    "    logistic_lce(mapParams=params, nIterates=TIME_SERIES_LENGTH, includeTrajectory=True)\n",
    "    for params in logistic_control_params\n",
    "]\n",
    "logistic_trajectories = [\n",
    "    data[\"trajectory\"][:,0]\n",
    "    for data in logistic_dataset\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hénon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "henon_control_params = [\n",
    "    dict(a=a, b=0.3) for a in np.sort(randomState.uniform(0.8, 1.4, NUM_CONTROL_PARAM_SAMPLES))\n",
    "]\n",
    "henon_dataset = [\n",
    "    henon_lce(mapParams=params, nIterates=TIME_SERIES_LENGTH, includeTrajectory=True)\n",
    "    for params in henon_control_params\n",
    "]\n",
    "henon_trajectories = [\n",
    "    data[\"trajectory\"][:,0]\n",
    "    for data in henon_dataset\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ikeda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ikeda_control_params = [\n",
    "    dict(a=a) for a in np.sort(randomState.uniform(0.5, 1.0, NUM_CONTROL_PARAM_SAMPLES))\n",
    "]\n",
    "ikeda_dataset = [\n",
    "    ikeda_lce(mapParams=params, nIterates=TIME_SERIES_LENGTH, includeTrajectory=True)\n",
    "    for params in ikeda_control_params\n",
    "]\n",
    "ikeda_trajectories = [\n",
    "    data[\"trajectory\"][:,0]\n",
    "    for data in ikeda_dataset\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tinkerbell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tinkerbell_control_params = [\n",
    "    dict(a=a) for a in np.sort(randomState.uniform(0.7, 0.9, NUM_CONTROL_PARAM_SAMPLES))\n",
    "]\n",
    "tinkerbell_dataset = [\n",
    "    tinkerbell_lce(mapParams=params, nIterates=TIME_SERIES_LENGTH, includeTrajectory=True)\n",
    "    for params in tinkerbell_control_params\n",
    "]\n",
    "tinkerbell_trajectories = [\n",
    "    data[\"trajectory\"][:,0]\n",
    "    for data in tinkerbell_dataset\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build time series representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_representation(dataset, rep_class, rep_class_kwargs):\n",
    "    trajectories = [data[\"trajectory\"][:,0] for data in dataset]\n",
    "    return [rep_class(ts, **rep_class_kwargs) for ts in trajectories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tshvg_kwargs = dict(\n",
    "    DEGREE_DISTRIBUTION_MAX_DEGREE=100,\n",
    "    DEGREE_DISTRIBUTION_DIVERGENCE_P_VALUE=1.0,\n",
    "    directed=None,\n",
    "    weighted=None,\n",
    "    penetrable_limit=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsmt_kwargs = dict(\n",
    "    INTERLEAVING_DIVERGENCE_MESH=0.5,\n",
    "    DMT_ALPHA=0.5,\n",
    "    DISTRIBUTION_VECTOR_LENGTH=100,\n",
    "    LEAF_NEIGHBOUR_OFFSET=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsph_kwargs = dict(\n",
    "    ENTROPY_SUMMARY_RESOLUTION=100,\n",
    "    BETTI_CURVE_RESOLUTION=100,\n",
    "    BETTI_CURVE_NORM_P_VALUE=1.0,\n",
    "    SILHOUETTE_RESOLUTION=100,\n",
    "    SILHOUETTE_WEIGHT=1,\n",
    "    LIFESPAN_CURVE_RESOLUTION=100,\n",
    "    IMAGE_BANDWIDTH=0.2,\n",
    "    IMAGE_RESOLUTION=20,\n",
    "    ENTROPY_SUMMARY_DIVERGENCE_P_VALUE=2.0,\n",
    "    PERSISTENCE_STATISTICS_DIVERGENCE_P_VALUE=2.0,\n",
    "    WASSERSTEIN_DIVERGENCE_P_VALUE=1.0,\n",
    "    BETTI_CURVE_DIVERGENCE_P_VALUE=1.0,\n",
    "    PERSISTENCE_SILHOUETTE_DIVERGENCE_P_VALUE=2.0,\n",
    "    PERSISTENCE_LIFESPAN_DIVERGENCE_P_VALUE=2.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_tshvgs = build_representation(logistic_dataset, TSHVG, tshvg_kwargs)\n",
    "logistic_tsmts = build_representation(logistic_dataset, TSMT, tsmt_kwargs)\n",
    "logistic_tsphs = build_representation(logistic_dataset, TSPH, tsph_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hénon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "henon_tshvgs = build_representation(henon_dataset, TSHVG, tshvg_kwargs)\n",
    "henon_tsmts = build_representation(henon_dataset, TSMT, tsmt_kwargs)\n",
    "henon_tsphs = build_representation(henon_dataset, TSPH, tsph_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ikeda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ikeda_tshvgs = build_representation(ikeda_dataset, TSHVG, tshvg_kwargs)\n",
    "ikeda_tsmts = build_representation(ikeda_dataset, TSMT, tsmt_kwargs)\n",
    "ikeda_tsphs = build_representation(ikeda_dataset, TSPH, tsph_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tinkerbell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tinkerbell_tshvgs = build_representation(tinkerbell_dataset, TSHVG, tshvg_kwargs)\n",
    "tinkerbell_tsmts = build_representation(tinkerbell_dataset, TSMT, tsmt_kwargs)\n",
    "tinkerbell_tsphs = build_representation(tinkerbell_dataset, TSPH, tsph_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Lyapunov exponents and topological divergences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lyapunov exponents (ground truth)\n",
    "\n",
    "Calculated using numerical integration and the Benettin algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_lces = np.array([data[\"lce\"][0] for data in logistic_dataset])\n",
    "henon_lces = np.array([data[\"lce\"][0] for data in henon_dataset])\n",
    "ikeda_lces = np.array([data[\"lce\"][0] for data in ikeda_dataset])\n",
    "tinkerbell_lces = np.array([data[\"lce\"][0] for data in tinkerbell_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.10634193, -0.06317295,  0.39735755,  0.42847687,  0.43455378,\n",
       "       -0.13680436, -0.01506717,  0.43273952, -0.07622925,  0.55053153])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_lces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_of_arrays(list_of_dicts):\n",
    "    \"\"\"Convert list of dictionaries with equal keys to a dictionary of numpy arrays.\n",
    "    \n",
    "    Example\n",
    "        Input\n",
    "            [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]\n",
    "        Output\n",
    "            {'a': np.array([1, 3]), 'b': np.array([2, 4])}\n",
    "    \"\"\"\n",
    "    return {key: np.array([d[key] for d in list_of_dicts]) for key in list_of_dicts[0]}\n",
    "\n",
    "def topological_divergences(ts_representations):\n",
    "    divergences = dv.map_sync(lambda rep: rep.divergences, ts_representations)\n",
    "    return dict_of_arrays(divergences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HVG divergences\n",
    "\n",
    "Wasserstein and $L_p$ divergences of the time series HVG degree distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_hvg_divergences = topological_divergences(logistic_tshvgs)\n",
    "henon_hvg_divergences = topological_divergences(henon_tshvgs)\n",
    "ikeda_hvg_divergences = topological_divergences(ikeda_tshvgs)\n",
    "tinkerbell_hvg_divergences = topological_divergences(tinkerbell_tshvgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'degree_wasserstein': array([0.00029851, 0.00029851, 0.00139303, 0.00109453, 0.00079602,\n",
       "        0.00039801, 0.000199  , 0.00109453, 0.0039801 , 0.00169154]),\n",
       " 'degree_lp': array([0.039801  , 0.7761194 , 0.17910448, 0.24875622, 0.16915423,\n",
       "        0.039801  , 0.0199005 , 0.17910448, 0.7761194 , 0.21890547])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_hvg_divergences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge tree divergences\n",
    "\n",
    "Interleaving divergence and leaf-to-offset-leaf path length distribution divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_mt_divergences = topological_divergences(logistic_tsmts)\n",
    "henon_mt_divergences = topological_divergences(henon_tsmts)\n",
    "ikeda_mt_divergences = topological_divergences(ikeda_tsmts)\n",
    "tinkerbell_mt_divergences = topological_divergences(tinkerbell_tsmts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'interleaving': array([3.93313734e-01, 6.15667548e-01, 6.88947523e-01, 7.55338508e-01,\n",
       "        7.67488701e-01, 1.91288674e-08, 8.10931079e-01, 8.20755652e-01,\n",
       "        7.58843195e-01, 8.79628651e-01]),\n",
       " 'leaf_to_leaf_path_length': array([0.00059394, 0.00049051, 0.00233158, 0.00088515, 0.00132852,\n",
       "        0.00029851, 0.0003166 , 0.00142857, 0.00074051, 0.00120482])}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_mt_divergences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistent homology divergences\n",
    "\n",
    "Various divergences based on the superlevel and sublevel persistence diagrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_ph_divergences = topological_divergences(logistic_tsphs)\n",
    "henon_ph_divergences = topological_divergences(henon_tsphs)\n",
    "ikeda_ph_divergences = topological_divergences(ikeda_tsphs)\n",
    "tinkerbell_ph_divergences = topological_divergences(tinkerbell_tsphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'point_summary_entropy': array([5.21179913e-04, 1.93738326e-03, 3.08965474e-03, 7.07017585e-04,\n",
       "        2.45777300e-03, 7.12452319e-12, 1.29340781e-03, 1.49321068e-03,\n",
       "        4.43695853e-04, 1.50770478e-03]),\n",
       " 'point_summary_max_persistence_ratio': array([6.68237354e-10, 3.73538755e-10, 2.13077875e-06, 3.29878903e-07,\n",
       "        7.41441419e-06, 1.21846977e-11, 1.60118122e-07, 6.46471089e-08,\n",
       "        4.37202485e-09, 3.52424915e-06]),\n",
       " 'point_summary_homology_class_ratio': array([0.00497512, 0.00497512, 0.00497512, 0.00497512, 0.00497512,\n",
       "        0.        , 0.00497512, 0.        , 0.00497512, 0.        ]),\n",
       " 'entropy': array([0.27211655, 0.46292688, 0.47501362, 0.40765834, 0.38399686,\n",
       "        0.06347962, 0.16911732, 0.40261831, 0.38422877, 0.44901553]),\n",
       " 'betti': array([100., 100.,  85.,  97.,  72.,   0.,  42.,  40., 100.,  87.]),\n",
       " 'silhouette': array([9.00514975e-03, 3.37850710e-02, 3.51090577e-02, 2.54191087e-02,\n",
       "        3.32253555e-02, 6.35089870e-09, 1.92827642e-02, 3.48520423e-02,\n",
       "        2.15674629e-02, 2.52391424e-02]),\n",
       " 'lifespan': array([4.0432509 , 4.27763145, 4.40909465, 5.55473545, 3.53572804,\n",
       "        0.80850745, 2.16554165, 4.87434644, 6.74956689, 5.69177888]),\n",
       " 'stats': array([1.00649734e+00, 1.08251758e+00, 1.00012163e+00, 1.00099354e+00,\n",
       "        1.00035473e+00, 1.20966896e-09, 1.22858435e+00, 2.13140788e-02,\n",
       "        1.04401802e+00, 9.42314789e-02]),\n",
       " 'entropy_EMD': array([0.01471457, 0.02791203, 0.02849391, 0.02489731, 0.02477212,\n",
       "        0.0006348 , 0.01626216, 0.01942768, 0.02227983, 0.02283425]),\n",
       " 'betti_EMD': array([0.78, 0.74, 0.55, 0.71, 0.62, 0.  , 0.42, 0.28, 0.72, 0.47]),\n",
       " 'silhouette_EMD': array([7.13749258e-04, 2.19504789e-03, 2.32077066e-03, 1.71505541e-03,\n",
       "        2.44886774e-03, 5.53270316e-10, 1.50737465e-03, 2.25411900e-03,\n",
       "        1.34771346e-03, 1.57220659e-03]),\n",
       " 'lifespan_EMD': array([0.32012393, 0.26639635, 0.27094363, 0.44824947, 0.22040558,\n",
       "        0.00808508, 0.13877542, 0.34521522, 0.53125423, 0.40268831]),\n",
       " 'stats_EMD': array([3.32335838e-02, 4.21271604e-02, 2.77072349e-02, 3.02388706e-02,\n",
       "        2.91296782e-02, 5.82957683e-11, 6.19231933e-02, 2.22347403e-03,\n",
       "        4.33550812e-02, 5.25728416e-03]),\n",
       " 'bottleneck': array([1.52756441e-01, 2.67823269e-01, 7.05012312e-02, 8.99484023e-02,\n",
       "        8.43139203e-02, 8.04189049e-10, 1.69001440e-01, 3.94310561e-02,\n",
       "        3.79421046e-01, 1.70918007e-01]),\n",
       " 'wasserstein': array([2.85568750e-01, 4.41744959e-01, 4.59940852e-01, 4.94033002e-01,\n",
       "        4.29190633e-01, 1.43533962e-09, 1.69709980e-01, 2.76331510e-01,\n",
       "        5.07311451e-01, 5.67767999e-01])}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_ph_divergences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "\n",
    "Other measures that might approximate or estimate the largest Lyapunov exponent of the trajectory data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical measures\n",
    "\n",
    "The Rosenstein and Eckmann estimates from Python `nolds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_rosenstein_estimates = dv.map_sync(lyap_r, logistic_trajectories)\n",
    "henon_rosenstein_estimates = dv.map_sync(lyap_r, henon_trajectories)\n",
    "ikeda_rosenstein_estimates = dv.map_sync(lyap_r, ikeda_trajectories)\n",
    "tinkerbell_rosenstein_estimates = dv.map_sync(lyap_r, tinkerbell_trajectories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.060033527173494014,\n",
       " -0.0301349970759175,\n",
       " -0.01609325408935554,\n",
       " 0.002687146792062013,\n",
       " 0.006097245570133959,\n",
       " 0.0003380567505397046,\n",
       " -0.0003902937019019587,\n",
       " -0.052557697152732764,\n",
       " 0.000643072041158883,\n",
       " 0.0006139790982520421]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_rosenstein_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_eckmann_estimates = np.array([x[0] for x in dv.map_sync(lyap_e, logistic_trajectories)])\n",
    "henon_eckmann_estimates = np.array([x[0] for x in dv.map_sync(lyap_e, henon_trajectories)])\n",
    "ikeda_eckmann_estimates = np.array([x[0] for x in dv.map_sync(lyap_e, ikeda_trajectories)])\n",
    "tinkerbell_eckmann_estimates = np.array([x[0] for x in dv.map_sync(lyap_e, tinkerbell_trajectories)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.15823905,  0.22930793,  0.69618255,  0.84920216,  0.93647736,\n",
       "        0.9846051 ,  0.89982027, -0.09447172,  0.30277407,  0.25711146],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_eckmann_estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HVG-based measures\n",
    "\n",
    "The $L_1$ distance between degree distributions of top and bottom HVGs as used in the _Peak vs pit asymmetry_ paper in **Scientific Reports**.\n",
    "\n",
    "This is already computed above as `logistic_hvg_divergences[\"degree_lp\"]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TDA-based measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chaos-chapter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
